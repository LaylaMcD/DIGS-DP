{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMskAgeHsEPe3roMtZIcyA0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LaylaMcD/DIGS-DP/blob/main/Word2Vec_Pt_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VSDXj8r3mRPm"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "import gensim\n",
        "import logging\n",
        "import glob, os\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from pprint import pprint\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import TfidfModel\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5RpTl4CnBoS",
        "outputId": "edea04be-7e86-4ee5-8738-85454b935626"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# directory containing all source texts for training the model\n",
        "data_dir=\"/content/drive/My Drive/Colab Notebooks/output_files\"\n",
        "os.chdir(data_dir)\n",
        "\n",
        "data = []\n",
        "for filename in glob.glob(\"*.txt\"):\n",
        "    filedata = open(filename, 'r').read()\n",
        "    data.append(filedata)\n",
        "\n",
        "print(\"Number of documents: \" + str(len(data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wc0jEXigQskj",
        "outputId": "6a1c00c7-51a2-4c5c-a200-c1ca20877ada"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download(\"popular\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSQTdr0KMcAb",
        "outputId": "7d4d9536-ddef-4011-8a46-b553ce86a214"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "from gensim import corpora, models\n",
        "#from contractions import CONTRACTION_MAP\n",
        "import re, string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "def tokenize_text(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "def pos_tag_text(text):\n",
        "    # convert Penn treebank tag to wordnet tag\n",
        "    def penn_to_wn_tags(pos_tag):\n",
        "        if pos_tag.startswith('J'):\n",
        "            return wn.ADJ\n",
        "        elif pos_tag.startswith('V'):\n",
        "            return wn.VERB\n",
        "        elif pos_tag.startswith('N'):\n",
        "            return wn.NOUN\n",
        "        elif pos_tag.startswith('R'):\n",
        "            return wn.ADV\n",
        "        else:\n",
        "            return None\n",
        "    tagged_text = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "    #tagged_text = tag(text)\n",
        "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag)) for word, pos_tag in tagged_text]\n",
        "    return tagged_lower_text\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    pos_tagged_text = pos_tag_text(text)\n",
        "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag else word for word, pos_tag in pos_tagged_text]\n",
        "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
        "    return lemmatized_text\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    tokens = tokenize_text(text)\n",
        "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
        "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stopword_list = nltk.corpus.stopwords.words('english')\n",
        "    tokens = tokenize_text(text)\n",
        "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "\n",
        "def normalize_corpus(corpus, tokenize):\n",
        "    normalized_corpus = []\n",
        "    for text in corpus:\n",
        "        text = lemmatize_text(text)\n",
        "        text = remove_special_characters(text)\n",
        "        text = remove_stopwords(text)\n",
        "        if tokenize:\n",
        "            text = tokenize_text(text)\n",
        "        normalized_corpus.append(text)\n",
        "    return normalized_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRjF1ePRMrRg",
        "outputId": "15a79c0a-5386-429d-a4a0-251656b5cf3b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "norm_corpus = normalize_corpus(data, tokenize=True)\n",
        "\n",
        "dictionary = Dictionary(norm_corpus)   # Build the dictionary\n",
        "\n",
        "# Convert to vector corpus\n",
        "vectors = [dictionary.doc2bow(text) for text in norm_corpus]\n",
        "\n",
        "# Build TF-IDF model\n",
        "tfidf = TfidfModel(vectors)"
      ],
      "metadata": {
        "id": "xHwJv0y7Mxfc"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get TF-IDF weights for 50 most document-specific terms\n",
        "weights = tfidf[vectors[0]]\n",
        "\n",
        "# Get terms from the dictionary and pair with weights\n",
        "weights = [(dictionary[pair[0]], pair[1]) for pair in weights]\n",
        "\n",
        "pprint(sorted(weights, key=lambda weights: weights[1], reverse=True)[1:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOul8R5mM1C4",
        "outputId": "6a320dc3-524c-4391-e348-22161bcec560"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('snbts', 0.37114033360913157),\n",
            " ('haemophilia', 0.3057580986728079),\n",
            " ('liberton', 0.2346068430068086),\n",
            " ('z8', 0.16537859425070114),\n",
            " ('ludlam', 0.1384564975122149),\n",
            " ('berühmte', 0.136533490602323),\n",
            " ('basler', 0.12884146296275553),\n",
            " ('ewp', 0.10384237313416117),\n",
            " ('cryoprecipitate', 0.09812106281460813),\n",
            " ('dpc', 0.0942273385847018),\n",
            " ('swara', 0.09230433167480993),\n",
            " ('warmest', 0.08461230403524243),\n",
            " ('fviii', 0.07884328330556681),\n",
            " ('lta', 0.07692027639567493),\n",
            " ('wettest', 0.07692027639567493),\n",
            " ('d800', 0.0725827039998471),\n",
            " ('sanskaar', 0.07115125566599932),\n",
            " ('ihre', 0.06720620740726584),\n",
            " ('19712000', 0.0653822349363237),\n",
            " ('egfr', 0.0653822349363237),\n",
            " ('shhd', 0.06345922802643184),\n",
            " ('8y', 0.05961321420664808),\n",
            " ('buckalew', 0.05576720038686433),\n",
            " ('wonmunna', 0.05576720038686433),\n",
            " ('cistern', 0.055109090073957985),\n",
            " ('transfusion', 0.054296969771322826),\n",
            " ('ius', 0.051921186567080585),\n",
            " ('ravitch', 0.04999817965718872),\n",
            " ('viii', 0.049385035532834014),\n",
            " ('mayne', 0.048075172747296835),\n",
            " ('catechism', 0.04704434518508609),\n",
            " ('zeit', 0.04704434518508609),\n",
            " ('bpl', 0.04615216583740497),\n",
            " ('suchnamed', 0.04615216583740497),\n",
            " ('brocolli', 0.043011972740650134),\n",
            " ('laksh', 0.038460138197837467),\n",
            " ('sunniest', 0.038460138197837467),\n",
            " ('sbwire', 0.03763547614806887),\n",
            " ('sieben', 0.03763547614806887),\n",
            " ('dw', 0.03629135199992355),\n",
            " ('1659', 0.03461412437805372),\n",
            " ('1dx', 0.03461412437805372),\n",
            " ('ackermann', 0.03461412437805372),\n",
            " ('appomattox', 0.03461412437805372),\n",
            " ('biographien', 0.03461412437805372),\n",
            " ('exon', 0.03461412437805372),\n",
            " ('franklinton', 0.03269111746816185),\n",
            " ('qv', 0.03269111746816185),\n",
            " ('interpol', 0.032175982086709826)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word_freq(search_term):\n",
        "    lemmacounts = []\n",
        "    for filename in glob.glob(\"*.txt\"):\n",
        "        filedata = open(filename, 'r').read()\n",
        "        tokens = word_tokenize(filedata)\n",
        "        lemmas = [nltk.stem.WordNetLemmatizer().lemmatize(token.lower()) for token in tokens]\n",
        "        lemmacounts.append([filename, lemmas.count(search_term)])\n",
        "    return lemmacounts\n",
        "\n",
        "word_freq(\"interpol\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BGL_9wfSPWT",
        "outputId": "02c555f9-fe69-4461-b5b2-6d86d1218b11"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['chunk_2.txt', 32],\n",
              " ['chunk_10.txt', 0],\n",
              " ['chunk_5.txt', 0],\n",
              " ['chunk_9.txt', 0],\n",
              " ['chunk_6.txt', 0],\n",
              " ['chunk_4.txt', 0],\n",
              " ['chunk_3.txt', 0],\n",
              " ['chunk_8.txt', 6],\n",
              " ['chunk_7.txt', 0],\n",
              " ['chunk_1.txt', 2]]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like Interpol appeared a couple of times! Let's find the text"
      ],
      "metadata": {
        "id": "mg6Fs-LUSeUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freq_check(filename, search_term):\n",
        "    filedata = open(filename, 'r').read()\n",
        "    tokens = word_tokenize(filedata)\n",
        "    lemmas = [nltk.stem.WordNetLemmatizer().lemmatize(token.lower()) for token in tokens]\n",
        "    resultstr = \"In file \" + filename + \", the term '\" + search_term + \"' exists \" + str(lemmas.count(search_term)) + \" times\" + \\\n",
        "    \" compared to \" + str(len(lemmas)) + \" total lemmatized tokens for a raw frequency of \" + str((lemmas.count(search_term) / len(lemmas)))\n",
        "    return resultstr\n",
        "\n",
        "freq_check('chunk_2.txt','interpol')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3btJv2BrSekV",
        "outputId": "8669f514-f7ca-4bdf-cf00-8168e2ceb793"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"In file chunk_2.txt, the term 'interpol' exists 32 times compared to 567929 total lemmatized tokens for a raw frequency of 5.634507130292695e-05\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What about in the other file?"
      ],
      "metadata": {
        "id": "XWsmjaV9Sq3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freq_check(filename, search_term):\n",
        "    filedata = open(filename, 'r').read()\n",
        "    tokens = word_tokenize(filedata)\n",
        "    lemmas = [nltk.stem.WordNetLemmatizer().lemmatize(token.lower()) for token in tokens]\n",
        "    resultstr = \"In file \" + filename + \", the term '\" + search_term + \"' exists \" + str(lemmas.count(search_term)) + \" times\" + \\\n",
        "    \" compared to \" + str(len(lemmas)) + \" total lemmatized tokens for a raw frequency of \" + str((lemmas.count(search_term) / len(lemmas)))\n",
        "    return resultstr\n",
        "\n",
        "freq_check('chunk_1.txt','interpol')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4EwF9c0CSqK8",
        "outputId": "d14f2623-d705-488c-94e0-ff86ce683ef6"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"In file chunk_1.txt, the term 'interpol' exists 2 times compared to 547212 total lemmatized tokens for a raw frequency of 3.654890609124069e-06\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's compare that word to the appearance of other words in the text files"
      ],
      "metadata": {
        "id": "cSLYq5UZS5N0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As a final check, provide the most frequent lemmas for that document (=file)\n",
        "from pprint import pprint\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# directory containing all source texts for training the model\n",
        "filename=\"chunk_1.txt\"\n",
        "\n",
        "filedata = open(filename, 'r').read()\n",
        "xtokens = word_tokenize(filedata)\n",
        "xlemmas = [nltk.stem.WordNetLemmatizer().lemmatize(token.lower()) for token in xtokens]\n",
        "\n",
        "# Stopwords and punctuation removal\n",
        "stop = stopwords.words('english') + list(string.punctuation)\n",
        "\n",
        "tokens_nsp = [i.lower() for i in xtokens if i.lower() not in stop]\n",
        "lemmas_nsp = [i.lower() for i in xlemmas if i.lower() not in stop]\n",
        "\n",
        "##\n",
        "\n",
        "print (\"\\nTokens:\")\n",
        "\n",
        "wfreq = []\n",
        "for w in set(tokens_nsp):\n",
        "    wfreq.append([w, tokens_nsp.count(w)])\n",
        "wfreq_sorted = sorted(wfreq, key=lambda wfreq: wfreq[1], reverse=True)\n",
        "pprint(wfreq_sorted[0:50])\n",
        "\n",
        "print (\"\\nLemmas:\")\n",
        "\n",
        "lfreq = []\n",
        "for w in set(lemmas_nsp):\n",
        "    lfreq.append([w, lemmas_nsp.count(w)])\n",
        "lfreq_sorted = sorted(lfreq, key=lambda lfreq: lfreq[1], reverse=True)\n",
        "pprint(lfreq_sorted[0:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5H63z-RS88i",
        "outputId": "62729224-306a-40fc-8365-afebc8a85520"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokens:\n",
            "[[\"''\", 4215],\n",
            " ['’', 3943],\n",
            " ['coloring', 2482],\n",
            " ['pages', 1817],\n",
            " ['``', 1584],\n",
            " [\"'s\", 1473],\n",
            " ['”', 1275],\n",
            " ['“', 1255],\n",
            " ['size', 1120],\n",
            " ['one', 1070],\n",
            " ['printable', 1039],\n",
            " ['like', 884],\n",
            " ['free', 837],\n",
            " ['time', 810],\n",
            " ['also', 806],\n",
            " ['new', 794],\n",
            " ['would', 775],\n",
            " ['get', 747],\n",
            " [\"n't\", 731],\n",
            " ['people', 657],\n",
            " ['said', 647],\n",
            " ['page', 615],\n",
            " ['first', 581],\n",
            " ['see', 579],\n",
            " ['may', 537],\n",
            " ['--', 525],\n",
            " ['make', 520],\n",
            " ['kids', 510],\n",
            " ['know', 503],\n",
            " ['best', 501],\n",
            " ['book', 488],\n",
            " ['well', 474],\n",
            " ['good', 465],\n",
            " ['day', 462],\n",
            " ['use', 446],\n",
            " ['year', 445],\n",
            " ['...', 438],\n",
            " ['two', 433],\n",
            " ['–', 428],\n",
            " ['even', 425],\n",
            " ['world', 419],\n",
            " ['us', 415],\n",
            " ['way', 413],\n",
            " ['many', 409],\n",
            " ['work', 409],\n",
            " ['1', 405],\n",
            " ['2', 401],\n",
            " ['great', 395],\n",
            " ['could', 394],\n",
            " ['love', 393]]\n",
            "\n",
            "Lemmas:\n",
            "[[\"''\", 4215],\n",
            " ['’', 3943],\n",
            " ['coloring', 2482],\n",
            " ['page', 2432],\n",
            " ['wa', 2218],\n",
            " ['``', 1584],\n",
            " [\"'s\", 1473],\n",
            " ['”', 1275],\n",
            " ['“', 1255],\n",
            " ['ha', 1173],\n",
            " ['size', 1136],\n",
            " ['one', 1130],\n",
            " ['printable', 1039],\n",
            " ['time', 960],\n",
            " ['like', 910],\n",
            " ['free', 839],\n",
            " ['year', 823],\n",
            " ['also', 806],\n",
            " ['get', 805],\n",
            " ['new', 794],\n",
            " ['would', 775],\n",
            " [\"n't\", 731],\n",
            " ['book', 678],\n",
            " ['people', 661],\n",
            " ['day', 654],\n",
            " ['said', 647],\n",
            " ['make', 645],\n",
            " ['see', 588],\n",
            " ['first', 581],\n",
            " ['kid', 561],\n",
            " ['know', 547],\n",
            " ['may', 540],\n",
            " ['--', 525],\n",
            " ['best', 501],\n",
            " ['work', 498],\n",
            " ['good', 489],\n",
            " ['way', 489],\n",
            " ['well', 476],\n",
            " ['u', 457],\n",
            " ['take', 456],\n",
            " ['need', 453],\n",
            " ['right', 448],\n",
            " ['use', 446],\n",
            " ['...', 438],\n",
            " ['state', 435],\n",
            " ['two', 433],\n",
            " ['–', 428],\n",
            " ['even', 425],\n",
            " ['world', 421],\n",
            " ['game', 420]]\n"
          ]
        }
      ]
    }
  ]
}